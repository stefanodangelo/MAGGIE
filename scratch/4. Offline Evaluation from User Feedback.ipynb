{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1247f1-cb4f-4e43-a91e-3d044b5376b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Turn the Review App logs into an Evaluation Set\n",
    "\n",
    "The Review application captures your user feedbacks.\n",
    "\n",
    "This feedback is saved under 2 tables within your schema.\n",
    "\n",
    "In this notebook, we will show you how to extract the logs from the Review App into an Evaluation Set.  It is important to review each row and ensure the data quality is high e.g., the question is logical and the response makes sense.\n",
    "\n",
    "1. Requests with a üëç :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, the model's generated response.\n",
    "2. Requests with a üëé :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, null.\n",
    "3. Requests without any feedback\n",
    "    - `request`: As entered by the user\n",
    "\n",
    "Across all types of requests, if the user üëç a chunk from the `retrieved_context`, the `doc_uri` of that chunk is included in `expected_retrieved_context` for the question.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=429769148865153&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585dc76d-7d85-40b0-a593-3fefba68d14c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-agents mlflow mlflow-skinny databricks-sdk==0.23.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./0. Init\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7fc89be-60c0-48b8-b60a-33f3b0ade275",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1.1/ Extracting the logs \n",
    "\n",
    "\n",
    "*Note: for now, this part requires a few SQL queries that we provide in this notebook to properly format the review app into training dataset.*\n",
    "\n",
    "*We'll update this notebook soon with an simpler version - stay tuned!*\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=429769148865153&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f586a852-a86d-4523-bfdd-ff77fb7a5c42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import mlflow\n",
    "\n",
    "browser_url = mlflow.utils.databricks_utils.get_browser_hostname()\n",
    "\n",
    "# # Get the name of the Inference Tables where logs are stored\n",
    "active_deployments = agents.list_deployments()\n",
    "active_deployment = next((item for item in active_deployments if item.model_name == MODEL_NAME_FQN), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f04dafe-d1eb-420c-af12-46128838e0bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "print(active_deployment)\n",
    "endpoint = w.serving_endpoints.get(active_deployment.endpoint_name)\n",
    "\n",
    "try:\n",
    "    endpoint_config = endpoint.config.auto_capture_config\n",
    "except AttributeError as e:\n",
    "    endpoint_config = endpoint.pending_config.auto_capture_config\n",
    "\n",
    "inference_table_name = endpoint_config.state.payload_table.name\n",
    "inference_table_catalog = endpoint_config.catalog_name\n",
    "inference_table_schema = endpoint_config.schema_name\n",
    "\n",
    "# Cleanly formatted tables\n",
    "assessment_table = f\"{inference_table_catalog}.{inference_table_schema}.`{inference_table_name}_assessment_logs`\"\n",
    "request_table = f\"{inference_table_catalog}.{inference_table_schema}.`{inference_table_name}_request_logs`\"\n",
    "\n",
    "# Note: you might have to wait a bit for the tables to be ready\n",
    "print(f\"Request logs: {request_table}\")\n",
    "requests_df = spark.table(request_table)\n",
    "print(f\"Assessment logs: {assessment_table}\")\n",
    "#Temporary helper to extract the table - see _resources/00-init-advanced \n",
    "assessment_df = deduplicate_assessments_table(assessment_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59459588-75f7-40b9-93de-389b8252b763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "requests_with_feedback_df = requests_df.join(assessment_df, requests_df.databricks_request_id == assessment_df.request_id, \"left\")\n",
    "display(requests_with_feedback_df.select(\"request_raw\", \"trace\", \"source\", \"text_assessment\", \"retrieval_assessments\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b9791d-0d1b-4e5a-9c15-68e74e00d657",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "requests_with_feedback_df.createOrReplaceTempView('latest_assessments')\n",
    "eval_dataset = spark.sql(f\"\"\"\n",
    "-- Thumbs up.  Use the model's generated response as the expected_response\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  r.response as expected_response,\n",
    "  'thumbs_up' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value == \"positive\"\n",
    "union all\n",
    "  --Thumbs down.  If edited, use that as the expected_response.\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  IF(\n",
    "    a.text_assessment.suggested_output != \"\",\n",
    "    a.text_assessment.suggested_output,\n",
    "    NULL\n",
    "  ) as expected_response,\n",
    "  'thumbs_down' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value = \"negative\"\n",
    "union all\n",
    "  -- No feedback.  Include the request, but no expected_response\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  IF(\n",
    "    a.text_assessment.suggested_output != \"\",\n",
    "    a.text_assessment.suggested_output,\n",
    "    NULL\n",
    "  ) as expected_response,\n",
    "  'no_feedback_provided' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value != \"negative\"\n",
    "  and a.text_assessment.ratings [\"answer_correct\"].value != \"positive\"\n",
    "  \"\"\")\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55550389-9355-4ad0-930f-cb2c985ff4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1.2/ Our eval dataset is now ready! \n",
    "\n",
    "The review app makes it easy to build & create your evaluation dataset. \n",
    "\n",
    "*Note: the eval app logs may take some time to be available to you. If the dataset is empty, wait a bit.*\n",
    "\n",
    "To simplify the demo and make sure you don't have to craft your own eval dataset, we saved a ready-to-use eval dataset already pre-generated for you. We'll use this one for the demo instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6515a782-2a24-49cd-8dd2-ee504c542236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = spark.table(\"hackathon_eval_set\").limit(10)\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b02cbeb-f16a-4a78-8dd3-d86b00c35377",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load the correct Python environment for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "652cf22b-c18f-4d8e-9034-d5546489f0c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve the model we want to eval\n",
    "model = get_latest_model(MODEL_NAME_FQN)\n",
    "pip_requirements = mlflow.pyfunc.get_model_dependencies(f\"runs:/{model.run_id}/chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ccae52a-f486-44b4-abb4-847dc8f77c8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run our evaluation from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4534ecf-03c9-4d8f-9724-58dd665c1de9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"hackathon_eval_dataset\"):\n",
    "    # Evaluate the logged model\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        model=f'runs:/{model.run_id}/chain',\n",
    "        model_type=\"databricks-agent\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c260e07-b93a-4fde-8d11-fab0fd9b692c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### This is looking good, let's tag our model as production ready\n",
    "\n",
    "After reviewing the model correctness and potentially comparing its behavior to your other previous version, we can flag our model as ready to be deployed.\n",
    "\n",
    "*Note: Evaluation can be automated and part of a MLOps step: once you deploy a new Chatbot version with a new prompt, run the evaluation job and benchmark your model behavior vs the previous version.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477af809-4834-43d8-8282-7b2cd6d4a0af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# client = MlflowClient()\n",
    "# client.set_registered_model_alias(name=MODEL_NAME_FQN, alias=\"prod\", version=model.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Offline Evaluation from User Feedback",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
